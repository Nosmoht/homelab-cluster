apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: capi-rollout-orchestrator
spec:
  serviceAccountName: capi-rollout-runner
  entrypoint: rollout
  synchronization:
    mutexes:
      - name: capi-rollout-{{workflow.parameters.clusterNamespace}}-{{workflow.parameters.clusterName}}
  ttlStrategy:
    secondsAfterSuccess: 86400
    secondsAfterFailure: 86400
  arguments:
    parameters:
      - name: clusterName
      - name: clusterNamespace
        value: default
      - name: pollInterval
        value: 30s
      - name: controlPlaneTimeout
        value: 45m
      - name: workersTimeout
        value: 60m
      - name: maxRemediations
        value: "2"
      - name: maxConcurrentUnavailable
        value: "1"
      - name: rebootMode
        value: powercycle
      - name: rebootTimeout
        value: 30m
      - name: linstorGuardEnabled
        value: "true"
      - name: linstorNamespace
        value: piraeus-datastore
      - name: linstorControllerSelector
        value: app.kubernetes.io/component=linstor-controller
      - name: linstorStoragePool
        value: lvm-thick
      - name: linstorEvacuationTimeout
        value: 45m
      - name: linstorSettleTimeout
        value: 30m
  templates:
    - name: rollout
      steps:
        - - name: rollout-control-plane
            template: rollout-scope
            arguments:
              parameters:
                - name: scope
                  value: controlplane
        - - name: rollout-workers
            template: rollout-scope
            arguments:
              parameters:
                - name: scope
                  value: workers
    - name: rollout-scope
      inputs:
        parameters:
          - name: scope
      container:
        image: alpine/kubectl:1.35.0
        command: ["/bin/sh", "-ceu"]
        args:
          - |
            CLUSTER_NAME="{{workflow.parameters.clusterName}}"
            CLUSTER_NAMESPACE="{{workflow.parameters.clusterNamespace}}"
            POLL_INTERVAL="{{workflow.parameters.pollInterval}}"
            MAX_REMEDIATIONS="{{workflow.parameters.maxRemediations}}"
            MAX_CONCURRENT_UNAVAILABLE="{{workflow.parameters.maxConcurrentUnavailable}}"
            REBOOT_MODE="{{workflow.parameters.rebootMode}}"
            REBOOT_TIMEOUT="{{workflow.parameters.rebootTimeout}}"
            LINSTOR_GUARD_ENABLED="$(printf '%s' "{{workflow.parameters.linstorGuardEnabled}}" | tr '[:upper:]' '[:lower:]')"
            LINSTOR_NAMESPACE="{{workflow.parameters.linstorNamespace}}"
            LINSTOR_CONTROLLER_SELECTOR="{{workflow.parameters.linstorControllerSelector}}"
            LINSTOR_STORAGE_POOL="{{workflow.parameters.linstorStoragePool}}"
            LINSTOR_EVAC_TIMEOUT_RAW="{{workflow.parameters.linstorEvacuationTimeout}}"
            LINSTOR_SETTLE_TIMEOUT_RAW="{{workflow.parameters.linstorSettleTimeout}}"
            SCOPE="{{inputs.parameters.scope}}"

            if [ -z "${CLUSTER_NAME}" ]; then
              echo "clusterName is required"
              exit 1
            fi
            case "${MAX_CONCURRENT_UNAVAILABLE}" in
              ''|*[!0-9]*)
                echo "maxConcurrentUnavailable must be a positive integer, got '${MAX_CONCURRENT_UNAVAILABLE}'"
                exit 1
                ;;
            esac
            if [ "${MAX_CONCURRENT_UNAVAILABLE}" -lt 1 ]; then
              echo "maxConcurrentUnavailable must be >= 1, got '${MAX_CONCURRENT_UNAVAILABLE}'"
              exit 1
            fi
            case "${LINSTOR_GUARD_ENABLED}" in
              true|false)
                ;;
              *)
                echo "linstorGuardEnabled must be true or false, got '${LINSTOR_GUARD_ENABLED}'"
                exit 1
                ;;
            esac

            duration_to_seconds() {
              value="$1"
              case "$value" in
                *h) echo $(( ${value%h} * 3600 )) ;;
                *m) echo $(( ${value%m} * 60 )) ;;
                *s) echo $(( ${value%s} )) ;;
                *) echo "$value" ;;
              esac
            }

            require_binary() {
              if ! command -v "$1" >/dev/null 2>&1; then
                echo "missing required binary: $1"
                exit 1
              fi
            }

            require_binary kubectl

            resolve_single() {
              label_selector="$1"
              resource="$2"
              name="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${resource}" -l "${label_selector}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
              count="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${resource}" -l "${label_selector}" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | wc -w | tr -d ' ')"
              if [ "$count" != "1" ]; then
                echo "expected exactly one ${resource} with selector ${label_selector}, found ${count}"
                kubectl -n "${CLUSTER_NAMESPACE}" get "${resource}" -l "${label_selector}" -o name || true
                exit 1
              fi
              echo "$name"
            }

            resolve_single_in_namespace() {
              namespace="$1"
              label_selector="$2"
              resource="$3"
              name="$(kubectl -n "${namespace}" get "${resource}" -l "${label_selector}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
              count="$(kubectl -n "${namespace}" get "${resource}" -l "${label_selector}" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | wc -w | tr -d ' ')"
              if [ "$count" != "1" ]; then
                echo "expected exactly one ${resource} in namespace ${namespace} with selector ${label_selector}, found ${count}"
                kubectl -n "${namespace}" get "${resource}" -l "${label_selector}" -o name || true
                exit 1
              fi
              echo "$name"
            }

            table_row_count() {
              awk -F'|' '
                function trim(s) { gsub(/^[[:space:]]+|[[:space:]]+$/, "", s); return s }
                /^\|/ {
                  first = trim($2)
                  if (first == "" || first == "ResourceName" || first ~ /^=+$/) {
                    next
                  }
                  count++
                }
                END { print count + 0 }
              '
            }

            linstor_unsettled_count() {
              awk -F'|' '
                function trim(s) { gsub(/^[[:space:]]+|[[:space:]]+$/, "", s); return s }
                /^\|/ {
                  resource = trim($2)
                  conns = trim($6)
                  state = trim($7)
                  if (resource == "" || resource == "ResourceName" || resource ~ /^=+$/) {
                    next
                  }
                  if ((conns != "" && conns != "Ok") || state ~ /Sync|DELETING/) {
                    count++
                  }
                }
                END { print count + 0 }
              '
            }

            run_linstor() {
              if [ -z "${LINSTOR_CONTROLLER_POD:-}" ]; then
                echo "LINSTOR controller pod is not initialized"
                exit 1
              fi
              kubectl -n "${LINSTOR_NAMESPACE}" exec "${LINSTOR_CONTROLLER_POD}" -- linstor "$@"
            }

            check_linstor_global_health() {
              if ! faulty_output="$(run_linstor resource list --faulty --pastable 2>&1)"; then
                echo "failed to query LINSTOR faulty resources"
                printf '%s\n' "${faulty_output}"
                exit 1
              fi
              faulty_count="$(printf '%s\n' "${faulty_output}" | table_row_count)"
              echo "linstor global health: faultyResources=${faulty_count}"
              if [ "${faulty_count}" -gt 0 ]; then
                echo "LINSTOR has faulty resources; refusing remediation"
                printf '%s\n' "${faulty_output}"
                exit 1
              fi
            }

            wait_linstor_node_evacuated() {
              node_name="$1"
              start_time="$(date +%s)"
              while true; do
                if ! output="$(run_linstor resource list --pastable --nodes "${node_name}" --props "StorPoolName=${LINSTOR_STORAGE_POOL}" 2>&1)"; then
                  echo "failed to query LINSTOR resources for node ${node_name}"
                  printf '%s\n' "${output}"
                  exit 1
                fi

                diskful_count="$(printf '%s\n' "${output}" | table_row_count)"
                echo "linstor evacuation status: node=${node_name} pool=${LINSTOR_STORAGE_POOL} diskfulReplicas=${diskful_count}"
                if [ "${diskful_count}" -eq 0 ]; then
                  break
                fi

                now="$(date +%s)"
                if [ $((now - start_time)) -ge "${LINSTOR_EVAC_TIMEOUT_SECONDS}" ]; then
                  echo "timeout waiting for LINSTOR evacuation on node ${node_name}"
                  printf '%s\n' "${output}"
                  exit 1
                fi

                sleep "${POLL_INTERVAL}"
              done
            }

            evacuate_linstor_node() {
              node_name="$1"
              echo "evacuating LINSTOR resources from node ${node_name}"
              if ! output="$(run_linstor node evacuate "${node_name}" 2>&1)"; then
                echo "linstor node evacuate failed for ${node_name}"
                printf '%s\n' "${output}"
                exit 1
              fi
              printf '%s\n' "${output}"
              wait_linstor_node_evacuated "${node_name}"
            }

            restore_linstor_node() {
              node_name="$1"
              start_time="$(date +%s)"
              while true; do
                if output="$(run_linstor node restore "${node_name}" 2>&1)"; then
                  printf '%s\n' "${output}"
                  break
                fi

                if echo "${output}" | grep -qi "not evicted"; then
                  echo "linstor node ${node_name} is not evicted; skipping restore"
                  break
                fi

                now="$(date +%s)"
                if [ $((now - start_time)) -ge "${LINSTOR_SETTLE_TIMEOUT_SECONDS}" ]; then
                  echo "timeout restoring LINSTOR node ${node_name}"
                  printf '%s\n' "${output}"
                  exit 1
                fi

                echo "linstor node restore retry for ${node_name}: ${output}"
                sleep "${POLL_INTERVAL}"
              done
            }

            wait_linstor_settled() {
              start_time="$(date +%s)"
              while true; do
                if ! output="$(run_linstor resource list --pastable 2>&1)"; then
                  echo "failed to query LINSTOR resource settle status"
                  printf '%s\n' "${output}"
                  exit 1
                fi

                unsettled_count="$(printf '%s\n' "${output}" | linstor_unsettled_count)"
                echo "linstor settle status: unsettledResources=${unsettled_count}"
                if [ "${unsettled_count}" -eq 0 ]; then
                  break
                fi

                now="$(date +%s)"
                if [ $((now - start_time)) -ge "${LINSTOR_SETTLE_TIMEOUT_SECONDS}" ]; then
                  echo "timeout waiting for LINSTOR to settle"
                  printf '%s\n' "${output}"
                  exit 1
                fi

                sleep "${POLL_INTERVAL}"
              done
            }

            TCP_NAME="$(resolve_single "cluster.x-k8s.io/cluster-name=${CLUSTER_NAME}" taloscontrolplane)"
            MD_NAME="$(resolve_single "cluster.x-k8s.io/cluster-name=${CLUSTER_NAME}" machinedeployment)"

            TALOS_SECRET_NAMES="$(kubectl -n "${CLUSTER_NAMESPACE}" get secret -l "cluster.x-k8s.io/cluster-name=${CLUSTER_NAME}" -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' | grep -- '-talosconfig$' || true)"
            TALOS_SECRET_COUNT="$(printf '%s' "$TALOS_SECRET_NAMES" | wc -w | tr -d ' ')"
            if [ "$TALOS_SECRET_COUNT" != "1" ]; then
              echo "expected exactly one talosconfig secret matching *-talosconfig for ${CLUSTER_NAMESPACE}/${CLUSTER_NAME}, found ${TALOS_SECRET_COUNT}"
              printf '%s\n' "$TALOS_SECRET_NAMES"
              exit 1
            fi
            TALOS_SECRET_NAME="$TALOS_SECRET_NAMES"

            TALOSCONFIG_FILE="$(mktemp)"
            trap 'rm -f "${TALOSCONFIG_FILE}"' EXIT
            kubectl -n "${CLUSTER_NAMESPACE}" get secret "${TALOS_SECRET_NAME}" -o jsonpath='{.data.talosconfig}' | base64 -d > "${TALOSCONFIG_FILE}"

            TALOS_VERSION="$(kubectl -n "${CLUSTER_NAMESPACE}" get taloscontrolplane "${TCP_NAME}" -o jsonpath='{.spec.controlPlaneConfig.controlplane.talosVersion}')"
            if [ -z "${TALOS_VERSION}" ]; then
              echo "failed to determine Talos version from TalosControlPlane"
              exit 1
            fi

            TALOSCTL_BIN="/tmp/talosctl"
            if ! command -v curl >/dev/null 2>&1 && ! command -v wget >/dev/null 2>&1; then
              echo "curl or wget required to download talosctl"
              exit 1
            fi
            TALOSCTL_URL="https://github.com/siderolabs/talos/releases/download/${TALOS_VERSION}/talosctl-linux-amd64"
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "${TALOSCTL_URL}" -o "${TALOSCTL_BIN}"
            else
              wget -O "${TALOSCTL_BIN}" "${TALOSCTL_URL}"
            fi
            chmod +x "${TALOSCTL_BIN}"

            # Safety checks
            TCP_MAX_SURGE="$(kubectl -n "${CLUSTER_NAMESPACE}" get taloscontrolplane "${TCP_NAME}" -o jsonpath='{.spec.rolloutStrategy.rollingUpdate.maxSurge}')"
            if [ "${TCP_MAX_SURGE}" != "0" ]; then
              echo "unsafe TalosControlPlane maxSurge=${TCP_MAX_SURGE}; expected 0"
              exit 1
            fi

            MD_MAX_SURGE="$(kubectl -n "${CLUSTER_NAMESPACE}" get machinedeployment "${MD_NAME}" -o jsonpath='{.spec.strategy.rollingUpdate.maxSurge}')"
            MD_MAX_UNAVAILABLE="$(kubectl -n "${CLUSTER_NAMESPACE}" get machinedeployment "${MD_NAME}" -o jsonpath='{.spec.strategy.rollingUpdate.maxUnavailable}')"
            if [ "${MD_MAX_SURGE}" != "0" ] || [ "${MD_MAX_UNAVAILABLE}" != "1" ]; then
              echo "unsafe MachineDeployment strategy maxSurge=${MD_MAX_SURGE} maxUnavailable=${MD_MAX_UNAVAILABLE}; expected 0/1"
              exit 1
            fi

            if [ "${SCOPE}" = "controlplane" ]; then
              TIMEOUT_RAW="{{workflow.parameters.controlPlaneTimeout}}"
              RESOURCE_KIND="taloscontrolplane"
              RESOURCE_NAME="${TCP_NAME}"
              MACHINE_SELECTOR="cluster.x-k8s.io/cluster-name=${CLUSTER_NAME},cluster.x-k8s.io/control-plane"
              USE_SPEC_UPTODATE="true"
            else
              TIMEOUT_RAW="{{workflow.parameters.workersTimeout}}"
              RESOURCE_KIND="machinedeployment"
              RESOURCE_NAME="${MD_NAME}"
              MACHINE_SELECTOR="cluster.x-k8s.io/cluster-name=${CLUSTER_NAME},cluster.x-k8s.io/deployment-name=${MD_NAME}"
              USE_SPEC_UPTODATE="false"
            fi

            TIMEOUT_SECONDS="$(duration_to_seconds "${TIMEOUT_RAW}")"
            if [ "${LINSTOR_GUARD_ENABLED}" = "true" ]; then
              if [ -z "${LINSTOR_NAMESPACE}" ] || [ -z "${LINSTOR_CONTROLLER_SELECTOR}" ] || [ -z "${LINSTOR_STORAGE_POOL}" ]; then
                echo "linstorNamespace, linstorControllerSelector and linstorStoragePool are required when linstorGuardEnabled=true"
                exit 1
              fi

              LINSTOR_EVAC_TIMEOUT_SECONDS="$(duration_to_seconds "${LINSTOR_EVAC_TIMEOUT_RAW}")"
              LINSTOR_SETTLE_TIMEOUT_SECONDS="$(duration_to_seconds "${LINSTOR_SETTLE_TIMEOUT_RAW}")"

              case "${LINSTOR_EVAC_TIMEOUT_SECONDS}" in
                ''|*[!0-9]*)
                  echo "linstorEvacuationTimeout must resolve to a positive duration, got '${LINSTOR_EVAC_TIMEOUT_RAW}'"
                  exit 1
                  ;;
              esac
              case "${LINSTOR_SETTLE_TIMEOUT_SECONDS}" in
                ''|*[!0-9]*)
                  echo "linstorSettleTimeout must resolve to a positive duration, got '${LINSTOR_SETTLE_TIMEOUT_RAW}'"
                  exit 1
                  ;;
              esac
              if [ "${LINSTOR_EVAC_TIMEOUT_SECONDS}" -lt 1 ] || [ "${LINSTOR_SETTLE_TIMEOUT_SECONDS}" -lt 1 ]; then
                echo "LINSTOR timeouts must be >= 1 second"
                exit 1
              fi

              LINSTOR_CONTROLLER_POD="$(resolve_single_in_namespace "${LINSTOR_NAMESPACE}" "${LINSTOR_CONTROLLER_SELECTOR}" pod)"
              echo "using LINSTOR controller pod ${LINSTOR_CONTROLLER_POD} in namespace ${LINSTOR_NAMESPACE}"
            fi
            START_TIME="$(date +%s)"
            REMEDIATIONS=0

            find_unhealthy_machine() {
              kubectl -n "${CLUSTER_NAMESPACE}" get machines -l "${MACHINE_SELECTOR}" \
                -o jsonpath='{range .items[*]}{.metadata.name}{"|"}{.status.phase}{"|"}{.status.conditions[?(@.type=="Ready")].status}{"|"}{.status.conditions[?(@.type=="NodeHealthy")].status}{"\n"}{end}' \
                | while IFS='|' read -r name phase ready nodehealthy; do
                    if [ -z "$name" ]; then
                      continue
                    fi
                    if [ "$phase" = "Failed" ] || [ "$ready" != "True" ] || [ "$nodehealthy" = "False" ]; then
                      echo "$name"
                      break
                    fi
                  done
            }

            machine_node_name() {
              machine="$1"
              kubectl -n "${CLUSTER_NAMESPACE}" get machine "${machine}" -o jsonpath='{.status.nodeRef.name}' 2>/dev/null || true
            }

            node_internal_ip() {
              node_name="$1"
              if [ -z "$node_name" ]; then
                echo ""
                return 0
              fi
              kubectl get node "${node_name}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || true
            }

            machine_internal_ip() {
              machine="$1"
              kubectl -n "${CLUSTER_NAMESPACE}" get machine "${machine}" -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null || true
            }

            check_concurrency_guard() {
              unavailable_count="$(kubectl -n "${CLUSTER_NAMESPACE}" get machines -l "${MACHINE_SELECTOR}" -o jsonpath='{range .items[*]}{.status.conditions[?(@.type=="Ready")].status}{"\n"}{end}' | awk '$1 != "True" { c++ } END { print c+0 }')"
              deleting_count="$(kubectl -n "${CLUSTER_NAMESPACE}" get machines -l "${MACHINE_SELECTOR}" -o jsonpath='{range .items[*]}{.metadata.deletionTimestamp}{"\n"}{end}' | awk 'NF { c++ } END { print c+0 }')"

              echo "guard ${RESOURCE_KIND}/${RESOURCE_NAME}: unavailable=${unavailable_count} deleting=${deleting_count} maxConcurrentUnavailable=${MAX_CONCURRENT_UNAVAILABLE}"

              if [ "${unavailable_count}" -gt "${MAX_CONCURRENT_UNAVAILABLE}" ]; then
                echo "guard violation: unavailable machines (${unavailable_count}) exceed maxConcurrentUnavailable (${MAX_CONCURRENT_UNAVAILABLE})"
                exit 1
              fi
              if [ "${deleting_count}" -gt "${MAX_CONCURRENT_UNAVAILABLE}" ]; then
                echo "guard violation: deleting machines (${deleting_count}) exceed maxConcurrentUnavailable (${MAX_CONCURRENT_UNAVAILABLE})"
                exit 1
              fi
            }

            wait_rollout() {
              while true; do
                GEN="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.metadata.generation}')"
                OBS="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.status.observedGeneration}' 2>/dev/null || echo "0")"
                REPLICAS="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")"
                READY="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")"
                if [ "${USE_SPEC_UPTODATE}" = "true" ]; then
                  UPDATED="${READY}"
                  SPEC_UPTODATE="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.status.conditions[?(@.type=="MachinesSpecUpToDate")].status}' 2>/dev/null || echo "")"
                else
                  UPDATED="$(kubectl -n "${CLUSTER_NAMESPACE}" get "${RESOURCE_KIND}" "${RESOURCE_NAME}" -o jsonpath='{.status.updatedReplicas}' 2>/dev/null || echo "0")"
                  SPEC_UPTODATE="True"
                fi

                REPLICAS=${REPLICAS:-0}
                UPDATED=${UPDATED:-0}
                READY=${READY:-0}
                OBS=${OBS:-0}

                echo "${RESOURCE_KIND}/${RESOURCE_NAME}: gen=${GEN} observed=${OBS} replicas=${REPLICAS} updated=${UPDATED} ready=${READY} specUpToDate=${SPEC_UPTODATE}"
                check_concurrency_guard

                if [ "$UPDATED" = "$REPLICAS" ] && [ "$READY" = "$REPLICAS" ] && [ "$OBS" = "$GEN" ] && [ "$SPEC_UPTODATE" = "True" ]; then
                  echo "${RESOURCE_KIND}/${RESOURCE_NAME} rollout complete"
                  break
                fi

                NOW="$(date +%s)"
                if [ $((NOW - START_TIME)) -ge "$TIMEOUT_SECONDS" ]; then
                  if [ "$REMEDIATIONS" -ge "$MAX_REMEDIATIONS" ]; then
                    echo "timeout reached and max remediations exhausted"
                    exit 1
                  fi

                  TARGET_MACHINE="$(find_unhealthy_machine)"
                  if [ -z "$TARGET_MACHINE" ]; then
                    echo "no unhealthy machine found for remediation"
                    exit 1
                  fi

                  echo "remediating machine ${TARGET_MACHINE}"
                  NODE_NAME="$(machine_node_name "${TARGET_MACHINE}")"
                  NODE_IP="$(node_internal_ip "${NODE_NAME}")"
                  if [ -z "${NODE_IP}" ]; then
                    NODE_IP="$(machine_internal_ip "${TARGET_MACHINE}")"
                  fi
                  if [ -z "${NODE_IP}" ]; then
                    echo "failed to determine internal IP for ${TARGET_MACHINE} (nodeRef=${NODE_NAME})"
                    exit 1
                  fi

                  if [ "${LINSTOR_GUARD_ENABLED}" = "true" ]; then
                    if [ -z "${NODE_NAME}" ]; then
                      echo "linstor guard requires nodeRef.name for ${TARGET_MACHINE}"
                      exit 1
                    fi
                    check_linstor_global_health
                    evacuate_linstor_node "${NODE_NAME}"
                  fi

                  echo "deleting machine ${TARGET_MACHINE}"
                  kubectl -n "${CLUSTER_NAMESPACE}" delete machine "${TARGET_MACHINE}" --wait=false --ignore-not-found=true

                  echo "rebooting node ${NODE_IP} via talosctl"
                  if ! output=$(${TALOSCTL_BIN} --talosconfig "${TALOSCONFIG_FILE}" -n "${NODE_IP}" reboot --mode="${REBOOT_MODE}" --wait=true --timeout "${REBOOT_TIMEOUT}" 2>&1); then
                    if echo "${output}" | grep -qi "certificate signed by unknown authority\|maintenance mode\|API is not implemented"; then
                      echo "node ${NODE_IP} in maintenance mode; manual action required"
                      exit 1
                    fi
                    echo "talosctl reboot failed: ${output}"
                    exit 1
                  fi

                  if [ "${LINSTOR_GUARD_ENABLED}" = "true" ]; then
                    restore_linstor_node "${NODE_NAME}"
                    wait_linstor_settled
                    check_linstor_global_health
                  fi

                  REMEDIATIONS=$((REMEDIATIONS + 1))
                  START_TIME="$(date +%s)"
                fi

                sleep "${POLL_INTERVAL}"
              done
            }

            wait_rollout
